<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Trainer - Conditional GAN</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Trainer";
        var mkdocs_page_input_path = "trainer.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> Conditional GAN
        </a>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../data_loader/">DataLoader</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../discriminator/">Discriminator</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../generator/">Generator</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Trainer</a>
    <ul class="current">
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../test/">Test</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../cli/">Command Line</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">Conditional GAN</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Trainer</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <div class="doc doc-object doc-module">



<a id="trainer"></a>
  <div class="doc doc-contents first">

  

  <div class="doc doc-children">








<div class="doc doc-object doc-class">



<h2 id="trainer.Trainer" class="doc doc-heading">
          <code>Trainer</code>


</h2>


  <div class="doc doc-contents ">

  
      <p>The <code>Trainer</code> class encapsulates the training process for a conditional Generative Adversarial Network (cGAN) composed of a generator and a discriminator. It manages the training loop, loss computations, and parameter updates.</p>
<h4 id="trainer.Trainer--attributes">Attributes:</h4>
<ul>
<li><code>latent_space</code> (int): Dimensionality of the generator's input latent space.</li>
<li><code>epochs</code> (int): Total number of training epochs.</li>
<li><code>learning_rate</code> (float): Learning rate for the Adam optimizers.</li>
<li><code>beta1</code> (float): Beta1 hyperparameter for the Adam optimizer.</li>
<li><code>beta2</code> (float): Beta2 hyperparameter for the Adam optimizer.</li>
<li><code>generator</code> (Generator): Generator model of the GAN.</li>
<li><code>discriminator</code> (Discriminator): Discriminator model of the GAN.</li>
<li><code>dataloader</code> (DataLoader): DataLoader providing the training data and labels.</li>
<li><code>loss_function</code> (nn.Module): Binary Cross Entropy loss function for training the discriminator and generator.</li>
<li><code>optimizer_generator</code> (optim.Optimizer): Optimizer for updating the generator's weights.</li>
<li><code>optimizer_discriminator</code> (optim.Optimizer): Optimizer for updating the discriminator's weights.</li>
<li><code>discriminator_loss</code> (list): List to record the discriminator's loss after each epoch.</li>
<li><code>generator_loss</code> (list): List to record the generator's loss after each epoch.</li>
</ul>
<h4 id="trainer.Trainer--methods">Methods:</h4>
<p>Detailed descriptions of each method are provided below.</p>

            <details class="quote">
              <summary>Source code in <code>trainer.py</code></summary>
              <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">class</span> <span class="nc">Trainer</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The `Trainer` class encapsulates the training process for a conditional Generative Adversarial Network (cGAN) composed of a generator and a discriminator. It manages the training loop, loss computations, and parameter updates.</span>

<span class="sd">    ## Attributes:</span>
<span class="sd">    - `latent_space` (int): Dimensionality of the generator&#39;s input latent space.</span>
<span class="sd">    - `epochs` (int): Total number of training epochs.</span>
<span class="sd">    - `learning_rate` (float): Learning rate for the Adam optimizers.</span>
<span class="sd">    - `beta1` (float): Beta1 hyperparameter for the Adam optimizer.</span>
<span class="sd">    - `beta2` (float): Beta2 hyperparameter for the Adam optimizer.</span>
<span class="sd">    - `generator` (Generator): Generator model of the GAN.</span>
<span class="sd">    - `discriminator` (Discriminator): Discriminator model of the GAN.</span>
<span class="sd">    - `dataloader` (DataLoader): DataLoader providing the training data and labels.</span>
<span class="sd">    - `loss_function` (nn.Module): Binary Cross Entropy loss function for training the discriminator and generator.</span>
<span class="sd">    - `optimizer_generator` (optim.Optimizer): Optimizer for updating the generator&#39;s weights.</span>
<span class="sd">    - `optimizer_discriminator` (optim.Optimizer): Optimizer for updating the discriminator&#39;s weights.</span>
<span class="sd">    - `discriminator_loss` (list): List to record the discriminator&#39;s loss after each epoch.</span>
<span class="sd">    - `generator_loss` (list): List to record the generator&#39;s loss after each epoch.</span>

<span class="sd">    ## Methods:</span>
<span class="sd">    Detailed descriptions of each method are provided below.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_space</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initializes the Trainer object with the specified configuration and sets up the neural network models, dataloader, loss function, and optimizers.</span>

<span class="sd">        ### Parameters:</span>
<span class="sd">        - `latent_space` (int): Size of the latent space (input vector for the generator).</span>
<span class="sd">        - `epochs` (int): Number of epochs for training the models.</span>
<span class="sd">        - `lr` (float): Learning rate for the Adam optimizers.</span>
<span class="sd">        - `beta1` (float): Beta1 hyperparameter for the Adam optimizer.</span>
<span class="sd">        - `beta2` (float): Beta2 hyperparameter for the Adam optimizer.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">latent_space</span> <span class="o">=</span> <span class="n">latent_space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;./data/processed/dataloader.pkl&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logging</span><span class="o">.</span><span class="n">exception</span><span class="p">(</span><span class="s2">&quot;dataloader is not transformed from pickle&quot;</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
            <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
            <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
            <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">saved_checkpoints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Saves a checkpoint of the given model at the specified epoch.</span>

<span class="sd">        ### Parameters:</span>
<span class="sd">        - `model` (nn.Module): The model to be saved.</span>
<span class="sd">        - `epoch` (int): The current epoch number for naming the saved file.</span>

<span class="sd">        ### Side Effects:</span>
<span class="sd">        - Saves the model&#39;s state dictionary to the file system.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
            <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;./models/checkpoints/generator_</span><span class="si">{}</span><span class="s2">.pth&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
        <span class="p">)</span>

    <span class="k">def</span> <span class="nf">train_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the discriminator model for one batch of data.</span>

<span class="sd">        ### Parameters (passed as keyword arguments):</span>
<span class="sd">        - `real_samples` (Tensor): Real samples from the dataset.</span>
<span class="sd">        - `labels` (Tensor): Corresponding labels for the real samples.</span>
<span class="sd">        - `fake_samples` (Tensor): Fake samples generated by the generator.</span>
<span class="sd">        - `real_labels` (Tensor): Tensor of ones, representing real labels.</span>
<span class="sd">        - `fake_labels` (Tensor): Tensor of zeros, representing fake labels.</span>

<span class="sd">        ### Returns:</span>
<span class="sd">        - `total_loss` (Tensor): The total loss for the discriminator for the current batch.</span>

<span class="sd">        ### Side Effects:</span>
<span class="sd">        - Updates the weights of the discriminator model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">real_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>
        <span class="n">fake_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fake_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>

        <span class="n">real_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">real_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_labels&quot;</span><span class="p">])</span>
        <span class="n">fake_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">fake_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fake_labels&quot;</span><span class="p">])</span>

        <span class="n">total_loss</span> <span class="o">=</span> <span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">total_loss</span>

    <span class="k">def</span> <span class="nf">train_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Trains the generator model for one batch of data.</span>

<span class="sd">        ### Parameters (passed as keyword arguments):</span>
<span class="sd">        - `generated_samples` (Tensor): Samples generated by the generator.</span>
<span class="sd">        - `real_labels` (Tensor): Tensor of ones, representing real labels.</span>
<span class="sd">        - `labels` (Tensor): Corresponding labels for the generated samples.</span>

<span class="sd">        ### Returns:</span>
<span class="sd">        - `generated_loss` (Tensor): The loss for the generator for the current batch.</span>

<span class="sd">        ### Side Effects:</span>
<span class="sd">        - Updates the weights of the generator model.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">generated_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span>
            <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;generated_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="n">generated_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">generated_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_labels&quot;</span><span class="p">])</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">generated_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">generated_loss</span>

    <span class="k">def</span> <span class="nf">train_CGAN</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Conducts the training loop for the Conditional Generative Adversarial Network (cGAN).</span>
<span class="sd">        The loop iterates over the dataset, trains the discriminator and generator in alternation,</span>
<span class="sd">        and records the loss for each epoch.</span>

<span class="sd">        ### Process:</span>
<span class="sd">        - For each epoch:</span>
<span class="sd">            - For each batch in the dataloader:</span>
<span class="sd">                - Train the discriminator using both real and fake data.</span>
<span class="sd">                - Generate new fake samples and train the generator.</span>
<span class="sd">                - Record and accumulate the loss for both the discriminator and generator.</span>
<span class="sd">        - After each epoch, print the average losses and save the generator&#39;s state as a checkpoint.</span>

<span class="sd">        ### Side Effects:</span>
<span class="sd">        - Updates the weights of both the discriminator and generator models.</span>
<span class="sd">        - Appends the average loss of each epoch to the respective loss lists (`discriminator_loss`, `generator_loss`).</span>
<span class="sd">        - Saves the generator&#39;s state after each epoch.</span>
<span class="sd">        - Prints the progress and average losses to the console.</span>

<span class="sd">        ### Error Handling:</span>
<span class="sd">        - If the model checkpoint cannot be saved, an exception is raised.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
            <span class="n">d_loss</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">g_loss</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">real_samples</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span>
                <span class="n">real_samples</span> <span class="o">=</span> <span class="n">real_samples</span>
                <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
                <span class="n">batch_size</span> <span class="o">=</span> <span class="n">real_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

                <span class="n">real_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
                <span class="n">fake_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

                <span class="n">noise_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_space</span><span class="p">)</span>
                <span class="n">fake_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">noise_samples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

                <span class="n">D_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_discriminator</span><span class="p">(</span>
                    <span class="n">real_labels</span><span class="o">=</span><span class="n">real_labels</span><span class="p">,</span>
                    <span class="n">fake_labels</span><span class="o">=</span><span class="n">fake_labels</span><span class="p">,</span>
                    <span class="n">fake_samples</span><span class="o">=</span><span class="n">fake_samples</span><span class="p">,</span>
                    <span class="n">real_samples</span><span class="o">=</span><span class="n">real_samples</span><span class="p">,</span>
                    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">generated_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">noise_samples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

                <span class="n">G_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_generator</span><span class="p">(</span>
                    <span class="n">generated_samples</span><span class="o">=</span><span class="n">generated_samples</span><span class="p">,</span>
                    <span class="n">real_labels</span><span class="o">=</span><span class="n">real_labels</span><span class="p">,</span>
                    <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="n">d_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">D_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
                <span class="n">g_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">G_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">d_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s2">] Completed&quot;</span><span class="p">)</span>
            <span class="nb">print</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;[==============] Average d_loss: </span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">d_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> - Average g_loss: </span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">saved_checkpoints</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Model cannot be saved successfully&quot;</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
            </details>

  

  <div class="doc doc-children">










<div class="doc doc-object doc-function">



<h3 id="trainer.Trainer.__init__" class="doc doc-heading">
          <code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="n">latent_space</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Initializes the Trainer object with the specified configuration and sets up the neural network models, dataloader, loss function, and optimizers.</p>
<h6 id="trainer.Trainer.__init__--parameters">Parameters:</h6>
<ul>
<li><code>latent_space</code> (int): Size of the latent space (input vector for the generator).</li>
<li><code>epochs</code> (int): Number of epochs for training the models.</li>
<li><code>lr</code> (float): Learning rate for the Adam optimizers.</li>
<li><code>beta1</code> (float): Beta1 hyperparameter for the Adam optimizer.</li>
<li><code>beta2</code> (float): Beta2 hyperparameter for the Adam optimizer.</li>
</ul>

          <details class="quote">
            <summary>Source code in <code>trainer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span>
<span class="normal">61</span>
<span class="normal">62</span>
<span class="normal">63</span>
<span class="normal">64</span>
<span class="normal">65</span>
<span class="normal">66</span>
<span class="normal">67</span>
<span class="normal">68</span>
<span class="normal">69</span>
<span class="normal">70</span>
<span class="normal">71</span>
<span class="normal">72</span>
<span class="normal">73</span>
<span class="normal">74</span>
<span class="normal">75</span>
<span class="normal">76</span>
<span class="normal">77</span>
<span class="normal">78</span>
<span class="normal">79</span>
<span class="normal">80</span>
<span class="normal">81</span>
<span class="normal">82</span>
<span class="normal">83</span>
<span class="normal">84</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">latent_space</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.0002</span><span class="p">,</span> <span class="n">beta1</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">beta2</span><span class="o">=</span><span class="mf">0.999</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Initializes the Trainer object with the specified configuration and sets up the neural network models, dataloader, loss function, and optimizers.</span>

<span class="sd">    ### Parameters:</span>
<span class="sd">    - `latent_space` (int): Size of the latent space (input vector for the generator).</span>
<span class="sd">    - `epochs` (int): Number of epochs for training the models.</span>
<span class="sd">    - `lr` (float): Learning rate for the Adam optimizers.</span>
<span class="sd">    - `beta1` (float): Beta1 hyperparameter for the Adam optimizer.</span>
<span class="sd">    - `beta2` (float): Beta2 hyperparameter for the Adam optimizer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">latent_space</span> <span class="o">=</span> <span class="n">latent_space</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta1</span> <span class="o">=</span> <span class="n">beta1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span> <span class="o">=</span> <span class="n">beta2</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">generator</span> <span class="o">=</span> <span class="n">Generator</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">Discriminator</span><span class="p">()</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">filename</span><span class="o">=</span><span class="s2">&quot;./data/processed/dataloader.pkl&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">exception</span><span class="p">(</span><span class="s2">&quot;dataloader is not transformed from pickle&quot;</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">BCELoss</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span>
        <span class="n">params</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
        <span class="n">lr</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">learning_rate</span><span class="p">,</span>
        <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta2</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="trainer.Trainer.saved_checkpoints" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">saved_checkpoints</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Saves a checkpoint of the given model at the specified epoch.</p>
<h6 id="trainer.Trainer.saved_checkpoints--parameters">Parameters:</h6>
<ul>
<li><code>model</code> (nn.Module): The model to be saved.</li>
<li><code>epoch</code> (int): The current epoch number for naming the saved file.</li>
</ul>
<h6 id="trainer.Trainer.saved_checkpoints--side-effects">Side Effects:</h6>
<ul>
<li>Saves the model's state dictionary to the file system.</li>
</ul>

          <details class="quote">
            <summary>Source code in <code>trainer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">86</span>
<span class="normal">87</span>
<span class="normal">88</span>
<span class="normal">89</span>
<span class="normal">90</span>
<span class="normal">91</span>
<span class="normal">92</span>
<span class="normal">93</span>
<span class="normal">94</span>
<span class="normal">95</span>
<span class="normal">96</span>
<span class="normal">97</span>
<span class="normal">98</span>
<span class="normal">99</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">saved_checkpoints</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Saves a checkpoint of the given model at the specified epoch.</span>

<span class="sd">    ### Parameters:</span>
<span class="sd">    - `model` (nn.Module): The model to be saved.</span>
<span class="sd">    - `epoch` (int): The current epoch number for naming the saved file.</span>

<span class="sd">    ### Side Effects:</span>
<span class="sd">    - Saves the model&#39;s state dictionary to the file system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span>
        <span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="s2">&quot;./models/checkpoints/generator_</span><span class="si">{}</span><span class="s2">.pth&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="trainer.Trainer.train_CGAN" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">train_CGAN</span><span class="p">()</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Conducts the training loop for the Conditional Generative Adversarial Network (cGAN).
The loop iterates over the dataset, trains the discriminator and generator in alternation,
and records the loss for each epoch.</p>
<h6 id="trainer.Trainer.train_CGAN--process">Process:</h6>
<ul>
<li>For each epoch:<ul>
<li>For each batch in the dataloader:<ul>
<li>Train the discriminator using both real and fake data.</li>
<li>Generate new fake samples and train the generator.</li>
<li>Record and accumulate the loss for both the discriminator and generator.</li>
</ul>
</li>
</ul>
</li>
<li>After each epoch, print the average losses and save the generator's state as a checkpoint.</li>
</ul>
<h6 id="trainer.Trainer.train_CGAN--side-effects">Side Effects:</h6>
<ul>
<li>Updates the weights of both the discriminator and generator models.</li>
<li>Appends the average loss of each epoch to the respective loss lists (<code>discriminator_loss</code>, <code>generator_loss</code>).</li>
<li>Saves the generator's state after each epoch.</li>
<li>Prints the progress and average losses to the console.</li>
</ul>
<h6 id="trainer.Trainer.train_CGAN--error-handling">Error Handling:</h6>
<ul>
<li>If the model checkpoint cannot be saved, an exception is raised.</li>
</ul>

          <details class="quote">
            <summary>Source code in <code>trainer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train_CGAN</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Conducts the training loop for the Conditional Generative Adversarial Network (cGAN).</span>
<span class="sd">    The loop iterates over the dataset, trains the discriminator and generator in alternation,</span>
<span class="sd">    and records the loss for each epoch.</span>

<span class="sd">    ### Process:</span>
<span class="sd">    - For each epoch:</span>
<span class="sd">        - For each batch in the dataloader:</span>
<span class="sd">            - Train the discriminator using both real and fake data.</span>
<span class="sd">            - Generate new fake samples and train the generator.</span>
<span class="sd">            - Record and accumulate the loss for both the discriminator and generator.</span>
<span class="sd">    - After each epoch, print the average losses and save the generator&#39;s state as a checkpoint.</span>

<span class="sd">    ### Side Effects:</span>
<span class="sd">    - Updates the weights of both the discriminator and generator models.</span>
<span class="sd">    - Appends the average loss of each epoch to the respective loss lists (`discriminator_loss`, `generator_loss`).</span>
<span class="sd">    - Saves the generator&#39;s state after each epoch.</span>
<span class="sd">    - Prints the progress and average losses to the console.</span>

<span class="sd">    ### Error Handling:</span>
<span class="sd">    - If the model checkpoint cannot be saved, an exception is raised.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">d_loss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">g_loss</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">real_samples</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">dataloader</span><span class="p">:</span>
            <span class="n">real_samples</span> <span class="o">=</span> <span class="n">real_samples</span>
            <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
            <span class="n">batch_size</span> <span class="o">=</span> <span class="n">real_samples</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="n">real_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">fake_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

            <span class="n">noise_samples</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">latent_space</span><span class="p">)</span>
            <span class="n">fake_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">noise_samples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

            <span class="n">D_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_discriminator</span><span class="p">(</span>
                <span class="n">real_labels</span><span class="o">=</span><span class="n">real_labels</span><span class="p">,</span>
                <span class="n">fake_labels</span><span class="o">=</span><span class="n">fake_labels</span><span class="p">,</span>
                <span class="n">fake_samples</span><span class="o">=</span><span class="n">fake_samples</span><span class="p">,</span>
                <span class="n">real_samples</span><span class="o">=</span><span class="n">real_samples</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">generated_samples</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">noise_samples</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

            <span class="n">G_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_generator</span><span class="p">(</span>
                <span class="n">generated_samples</span><span class="o">=</span><span class="n">generated_samples</span><span class="p">,</span>
                <span class="n">real_labels</span><span class="o">=</span><span class="n">real_labels</span><span class="p">,</span>
                <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="n">d_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">D_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
            <span class="n">g_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">G_loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">d_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator_loss</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch [</span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">epochs</span><span class="si">}</span><span class="s2">] Completed&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;[==============] Average d_loss: </span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">d_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> - Average g_loss: </span><span class="si">{</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">g_loss</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">saved_checkpoints</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="n">epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;Model cannot be saved successfully&quot;</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="trainer.Trainer.train_discriminator" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">train_discriminator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Trains the discriminator model for one batch of data.</p>
<h6 id="trainer.Trainer.train_discriminator--parameters-passed-as-keyword-arguments">Parameters (passed as keyword arguments):</h6>
<ul>
<li><code>real_samples</code> (Tensor): Real samples from the dataset.</li>
<li><code>labels</code> (Tensor): Corresponding labels for the real samples.</li>
<li><code>fake_samples</code> (Tensor): Fake samples generated by the generator.</li>
<li><code>real_labels</code> (Tensor): Tensor of ones, representing real labels.</li>
<li><code>fake_labels</code> (Tensor): Tensor of zeros, representing fake labels.</li>
</ul>
<h6 id="trainer.Trainer.train_discriminator--returns">Returns:</h6>
<ul>
<li><code>total_loss</code> (Tensor): The total loss for the discriminator for the current batch.</li>
</ul>
<h6 id="trainer.Trainer.train_discriminator--side-effects">Side Effects:</h6>
<ul>
<li>Updates the weights of the discriminator model.</li>
</ul>

          <details class="quote">
            <summary>Source code in <code>trainer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train_discriminator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains the discriminator model for one batch of data.</span>

<span class="sd">    ### Parameters (passed as keyword arguments):</span>
<span class="sd">    - `real_samples` (Tensor): Real samples from the dataset.</span>
<span class="sd">    - `labels` (Tensor): Corresponding labels for the real samples.</span>
<span class="sd">    - `fake_samples` (Tensor): Fake samples generated by the generator.</span>
<span class="sd">    - `real_labels` (Tensor): Tensor of ones, representing real labels.</span>
<span class="sd">    - `fake_labels` (Tensor): Tensor of zeros, representing fake labels.</span>

<span class="sd">    ### Returns:</span>
<span class="sd">    - `total_loss` (Tensor): The total loss for the discriminator for the current batch.</span>

<span class="sd">    ### Side Effects:</span>
<span class="sd">    - Updates the weights of the discriminator model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">real_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>
    <span class="n">fake_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fake_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">])</span>

    <span class="n">real_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">real_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_labels&quot;</span><span class="p">])</span>
    <span class="n">fake_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">fake_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;fake_labels&quot;</span><span class="p">])</span>

    <span class="n">total_loss</span> <span class="o">=</span> <span class="n">real_loss</span> <span class="o">+</span> <span class="n">fake_loss</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">total_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_discriminator</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">total_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>


<div class="doc doc-object doc-function">



<h3 id="trainer.Trainer.train_generator" class="doc doc-heading">
          <code class="highlight language-python"><span class="n">train_generator</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

</h3>


  <div class="doc doc-contents ">
  
      <p>Trains the generator model for one batch of data.</p>
<h6 id="trainer.Trainer.train_generator--parameters-passed-as-keyword-arguments">Parameters (passed as keyword arguments):</h6>
<ul>
<li><code>generated_samples</code> (Tensor): Samples generated by the generator.</li>
<li><code>real_labels</code> (Tensor): Tensor of ones, representing real labels.</li>
<li><code>labels</code> (Tensor): Corresponding labels for the generated samples.</li>
</ul>
<h6 id="trainer.Trainer.train_generator--returns">Returns:</h6>
<ul>
<li><code>generated_loss</code> (Tensor): The loss for the generator for the current batch.</li>
</ul>
<h6 id="trainer.Trainer.train_generator--side-effects">Side Effects:</h6>
<ul>
<li>Updates the weights of the generator model.</li>
</ul>

          <details class="quote">
            <summary>Source code in <code>trainer.py</code></summary>
            <div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span></span><span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span></pre></div></td><td class="code"><div><pre><span></span><code><span class="k">def</span> <span class="nf">train_generator</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Trains the generator model for one batch of data.</span>

<span class="sd">    ### Parameters (passed as keyword arguments):</span>
<span class="sd">    - `generated_samples` (Tensor): Samples generated by the generator.</span>
<span class="sd">    - `real_labels` (Tensor): Tensor of ones, representing real labels.</span>
<span class="sd">    - `labels` (Tensor): Corresponding labels for the generated samples.</span>

<span class="sd">    ### Returns:</span>
<span class="sd">    - `generated_loss` (Tensor): The loss for the generator for the current batch.</span>

<span class="sd">    ### Side Effects:</span>
<span class="sd">    - Updates the weights of the generator model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">generated_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span>
        <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;generated_samples&quot;</span><span class="p">],</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;labels&quot;</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">generated_loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss_function</span><span class="p">(</span><span class="n">generated_predict</span><span class="p">,</span> <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;real_labels&quot;</span><span class="p">])</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
    <span class="n">generated_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_generator</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">generated_loss</span>
</code></pre></div></td></tr></table></div>
          </details>
  </div>

</div>



  </div>

  </div>


</div>




  </div>

  </div>

</div>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../generator/" class="btn btn-neutral float-left" title="Generator"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../test/" class="btn btn-neutral float-right" title="Test">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../generator/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../test/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
